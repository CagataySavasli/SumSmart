{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:18.395881Z",
     "start_time": "2024-12-25T00:49:16.331525Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:21.262662Z",
     "start_time": "2024-12-25T00:49:18.399957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ],
   "id": "c3066b5b3be79616",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:21.332106Z",
     "start_time": "2024-12-25T00:49:21.329889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced rank\n",
    "    lora_alpha=8,  # Lower scaling factor\n",
    "    #target_modules=[\"q\"],  # Update fewer modules (e.g., only query weights)\n",
    "    lora_dropout=0.1,  # Increased dropout for better regularization\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Task type remains the same\n",
    ")"
   ],
   "id": "7dd06e920c1137bb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.633716Z",
     "start_time": "2024-12-25T00:49:21.375329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name='google/flan-t5-small'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
    "torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "7b072d1b7f9cbd16",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.644694Z",
     "start_time": "2024-12-25T00:49:22.642914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "ac0ec194d4efaef7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.693520Z",
     "start_time": "2024-12-25T00:49:22.688010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ],
   "id": "a88d3db5170e0a7c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.739578Z",
     "start_time": "2024-12-25T00:49:22.737006Z"
    }
   },
   "cell_type": "code",
   "source": "print(print_number_of_trainable_model_parameters(model))",
   "id": "4539ef52ee7f0f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.818029Z",
     "start_time": "2024-12-25T00:49:22.786514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_model = get_peft_model(model, \n",
    "                            lora_config)"
   ],
   "id": "138391d415581ee3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.839328Z",
     "start_time": "2024-12-25T00:49:22.836197Z"
    }
   },
   "cell_type": "code",
   "source": "print(print_number_of_trainable_model_parameters(peft_model))",
   "id": "b8ca899c9ad9f5e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 344064\n",
      "all model parameters: 77305216\n",
      "percentage of trainable model parameters: 0.45%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.886608Z",
     "start_time": "2024-12-25T00:49:22.883615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example"
   ],
   "id": "5b24c0dad49d4317",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:22.947601Z",
     "start_time": "2024-12-25T00:49:22.930602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ],
   "id": "9ff3d20bfc9e0a4f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:25.925666Z",
     "start_time": "2024-12-25T00:49:22.982309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 500 == 0, with_indices=True)\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "print(tokenized_datasets)"
   ],
   "id": "eff44a16f9d0dfde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c8aad42b9f7471d96306019e9a4c30d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa3359977a94954ab9043e03f30ea8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "325c986aaf6d4869956f576cfd72fa1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (25, 2)\n",
      "Validation: (1, 2)\n",
      "Test: (3, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T00:49:27.350214Z",
     "start_time": "2024-12-25T00:49:25.937364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = f'./SumSmart-training'\n",
    "peft_training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    max_seq_length=1024,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # per_device_eval_batch_size=3,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    # eval_strategy=\"epoch\",\n",
    "    optim=\"adamw_hf\",\n",
    "    #optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    )\n",
    "peft_trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    "    )"
   ],
   "id": "75174175f048e110",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagatay/.cache/pypoetry/virtualenvs/sumsmart-B6_iuGjy-py3.12/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-25T00:49:27.360551Z"
    }
   },
   "cell_type": "code",
   "source": "peft_trainer.train()",
   "id": "80c5e3818ada91d8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagatay/.cache/pypoetry/virtualenvs/sumsmart-B6_iuGjy-py3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/cagatay/.cache/pypoetry/virtualenvs/sumsmart-B6_iuGjy-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3618: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  ctx_manager = torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "peft_model_path=\"./SumSmart-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ],
   "id": "a203be9592baebc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "54ba1b6fbae5d38a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
